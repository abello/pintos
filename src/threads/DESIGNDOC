			+--------------------+
			|       CS 124       |
			| PROJECT 3: THREADS |
			|   DESIGN DOCUMENT  |
			+--------------------+
				   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Aleksander Bello <abello@caltech.edu>
Ronnel Boettcher <ronnel@caltech.edu>
Archan Luhar <archan@caltech.edu>

>> Specify how many late tokens you are using on this assignment:  
One.

>> What is the Git repository and commit hash for your submission?

   Repository URL:  
   commit ...

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course instructors.

			     ALARM CLOCK
			     ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.
 
struct thread_sleeping {
  /* Thread pointer. */
  struct thread *t;

  /* End time. Compare to timer_ticks(). */
  int64_t end_ticks;

  /* List element for sleep list */
  struct list_elem elem;
};

- Struct thread_sleeping is the data structure used to keep track of sleeping 
threads. End_ticks keeps track of when the thread should be awoken. 

static struct list sleep_list;

- sleep_list stores thread_sleeping structs, and is iterated over every clock
tick to check if there are any threads that need to be woken up.

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

- When timer_sleep() called, thread_sleep is called, and passed the value
timer_ticks() + ticks, and then we initialize a thread_sleeping struct with
and end value timer_ticks() + ticks. We store this struct in sleep_list. When
the timer interrupt handler takes us to thread_sleep(), we use the sleep_list
to wake up any threads whose end_ticks are less than timer_ticks(). Also, 
we check if the thread we have just woken up has priority greater than the
current running thread's priority, and we yield if the awoken thread's 
priority is greater. 

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

- sleep_list is kept as a sorted list, where threads that need to be woken up
earliest are stored closer to the beginning of the sleep_list. Thus, we simply
pop the first list_elem from the sleep_list to wake up the thread.

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

- We chose this design because it minimized the amount of time spend in
thread_tick(), since we only have to check the beginning of sleep_list to 
determine whether or not we have to wake up any threads. 

			 PRIORITY SCHEDULING
			 ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

struct thread {
    ...
    int donation_priority;              /*!< Donation priority (-1 if N/A). */
    ... 
    struct thread *donee;
    ...
};

- We added the above fields to struct thread. "donation_priority" keeps track 
of the current donated priority to this thread (-1 if no donations). "donee" 
is the thread this thread donates to (NULL if no donee). 

struct priority_donation_state {
    /* The lock that the current thread is seeking to unlock so that the 
     * thread running previously can resume execution.
     */
    struct lock *lock_desired;
    struct thread *donor;
    struct thread *donee;
    struct list_elem elem;              /*!< List element. */
};

- "priority_donation_state" keeps track of priority donations... most 
importantly for what desired lock the donation occurred. We also store
donor and donee. This is stored in "pri_donation_list".

extern struct list pri_donation_list;

- "pri_donation_list" stores priority_donation_state structs, and is used to 
store the information about a priority donation represented in the 
priority_donation_state.  

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)
   
   A                B               C
________         ________       ________
|      |        |      |        |      |
|  5   |------->|  2   |------->|   0  |
|______|        |______|        |______|

The ASCII art above demonstrates a nested donation donation scenario, where
the blocks represent threads, the number inside the block represents the 
priority of that thread, the arrows above representing the donor/donee
relation (donee has arrow pointing to it, donor has arrow leaving it), due to
the donor lacking the lock its donee has. In this situation, A would be 
running, because it has the highest priority. A would try to aquire lock a,
but B has lock a. It would then donate its priority to B. Now B has "effective
priority" 5. A would yield to B, letting B run. B would try to aquire lock b,
but C has lock b. Then, B would donate its priority to C, so then C's 
"effective priority" would now be 5. B would then yield to C, allowing C to 
run. C finishes, and its donation priority will now be -1 because it no longer
has any donations. Then B would run, acquire lock b, and finish, setting its 
donation priority to -1, since it no longer has any donations either. Now,
thread A, the original donor, is free to acquire lock a, and continue. 


---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

Thread yield is called at the end of the functions responsible for lifting
locks, semaphores and conditions, and since we maintain always that the ready
list remain sorted, threads that are unblocked will be placed in their correct
position in the ready_list, so that when yield is called, we can always pop
the first thread from the list, and be sure that it is the highest priority
thread.


>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

When lock_aquire() causes a priority donation, the following happens: The 
thread running creates a priority donation state struct, and stores the
lock it desires as well as the thread it is donating to in this struct,
then stores this in the pri_donation_list. We deal with nested donation 
through the following. Each time a thread donates it donates its 
"effective priority", which is the max of its donation priority and its normal
priority. Thus, if a thread donates to another thread whose priority is lower,
and that donee donates to another lower priority thread, it donates its 
"effective priority", and hence it will donate the original thread's priority
(which is the highest of all three threads). Also, there is the special case
we deal with when the thread we wish to donate to is blocked, in which case
this would indicate that the donee is waiting for a lock, so we would donate
the priority to the donee's donee (which we do by accessing the "donee" field
in the thread struct). This idea is refined by following the path of blocked
"donees" until we find an unblock donee who we can donate all the priority
to. Thus, this will handle the nested donations. 

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

When a lock is released in lock_release, we remove 
any priority_donation_state from pri_donation_list where lock_desired is the 
lock we are releasing, and the donee is the current thread. In this case, the
current thread running has "done its job" as a priority donee, by releasing 
the lock desired by a higher priority thread. After deleting these instances
from pri_donation_list, we iterate through the pri_donation_list again, to see
if the current running thread is a donee for any other threads, in which case
the current running thread will appear as some donee in the pri_donation_list.
We choose the largest donor out of these threads (if there are any), and set
the current thread's donation_priority equal to this largest value. If there
are no more donors for the current thread, donation_priority is set to its 
default value of -1. Then, we sema_up the semaphore for the lock we are 
releasing to unblock its waiters. In thread_unblock(), we ensure that the
newly unblocked threads are placed into their correct location in the sorted
ready_list. Thus, when thread_yield is called, we can be sure that the newly
unblocked thread with the highest priority will run next. 

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

We chose this design because it allows for any complicated forms of multiple
donation and nested donations, which we encounter in the tests. We considered
an idea before this one, of storing the previous priority (before the 
priority donation) in the priority_donation_state, and then when the correct
lock is released by the donee, the previous priority could be restored with
the value in the priority_donation_state. However, we realized this idea was
flawed, because it does not take into consideration multiple donations to the 
same thread. For example, if thread A donates to thread B, then thread C 
donates to thread B, under this method, the priority donation field of B would
be set to -1 after it had finished acquiring A's lock. However, what if C had
a larger donation to B than A did? Then B should should accept C's donation
before we run A again. Our updated design iterates through the donation list,
and checks if B is a donee for any other threads, and accepts the donation 
from the largest donor if one exists. 

There were several other design considerations that had to be carefully 
implemented, for example dealing with when we wanted to donate to a blocked
thread. In this case, it's not enough to donate to the blocked thread 
directly, because that thread wouldn't be able to use the donation (since its
blocked). Thus, we added a "donee" field to each thread, so that if we ever
encountered a blocked thread, we can follow the path of "donees", until we
find a thread that can run, and that we can donate to. This accomplishes the
original donor's desire to get its own lock released, by "helping out" the 
threads that hold its lock aquire their own desired locks. 

Thus, our design is robust enough to handle any amount of priority donation
and nested donations.

			  ADVANCED SCHEDULER
			  ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

struct thread {
    ...
    int niceness;           /*!< Between -20 and 20. */
    fixedpt recent_cpu;         /*!< CPU usage recently. */
    ...
};

- We added the niceness and recent_cpu fields because under the advanced 
scheduler each thread kept track of these values.

static fixedpt load_avg = 0;

- load_avg is a global variable to keep track of the load average for the 
threading system.

typedef int fixedpt;

- fixedpt was typedef to keep track of when we are using fixed points instead
of plain integers, so that we could use them with the various fixedpt 
functions (add, mul, div, etc.). 

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0      0   0   0  63  61  59     A
 4      4   0   0  62  61  59     A
 8      8   0   0  61  61  59     B 
12      8   4   0  61  60  59     A
16      12  4   0  60  60  59     B
20      12  8   0  60  59  59     A
24      16  8   0  59  59  59     B
28      16  12  0  59  58  59     C
32      16  12  4  59  58  58     A
36      20  12  4  58  58  58     B

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

There were some ambiguities, like for when timer ticks is 24, and all
of them were the same. In this situation, we rely on the round robin
scheduling to determine which thread to run next. Whenever threads are
placed in our ready_list, they are placed behind all threads of the same 
priority. This gives other threads with the same priority the chance to 
run.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

Most of the logic dealing with using the method in the advanced scheduler
was located inside the interrupt context. which seemed necessary to us
because many things in the advanced scheduler are based on time dependent.
This could slow performance because the thread_tick function is expected to
do a few operations then immediately be able to handle the next tick.

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

As stated in C4, all our code was handled in interrupt context, but this is 
because many calculations are time dependent. However, there are still a 
few speedups that we could have made to improve the code. The one major 
improvement would have been to keep the ready list sorted, so that in 
next_thread_to_run() we can quickly determine which thread to run next, 
rather than iterating through all threads and finding the max. However, this
is a tradeoff, because in thread_tick(), since we recalculate the priorities
of every thread, this implies that we would have had to sort the list in
thread_tick. Thus, it is somewhat of a tradeoff, although given enough 
time we probably could have gotten a faster, workable solution with a sorted
ready_list.

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?



			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the feedback survey on the course
website.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

This assignment was very difficult and took

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?
